{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077f2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-07 15:19:22.401478: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c577361",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/ckbj4p2n0jsdrxj9rs94ljf00000gn/T/ipykernel_1934/3333406404.py:2: DtypeWarning: Columns (1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  alphabets_df = pd.read_csv('alphabets_28x28.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "alphabets_df = pd.read_csv('alphabets_28x28.csv')\n",
    "sentiment_df = pd.read_csv('sentiment_analysis_dataset.csv')\n",
    "target_labels = pd.read_csv('target_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7704a83",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# OCR Model Training\n",
    "def train_ocr_model():\n",
    "\n",
    "    # Separate label column and pixel columns\n",
    "    labels = alphabets_df['label']\n",
    "    pixel_columns = alphabets_df.drop(columns=['label'])\n",
    "\n",
    "    # Convert pixel columns to numeric\n",
    "    pixel_columns = pixel_columns.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Combine labels and pixel columns back\n",
    "    alphabets_df_processed = pd.concat([labels, pixel_columns], axis=1)\n",
    "\n",
    "    # Drop rows with any NaNs\n",
    "    alphabets_df_processed = alphabets_df_processed.dropna()\n",
    "\n",
    "    # Drop duplicates\n",
    "    alphabets_df_processed = alphabets_df_processed.drop_duplicates()\n",
    "\n",
    "    # Only keep rows with valid labels (A-Z)\n",
    "    valid_labels = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "    alphabets_df_processed = alphabets_df_processed[alphabets_df_processed['label'].isin(valid_labels)]\n",
    "\n",
    "    if not alphabets_df_processed.empty:\n",
    "        # Prepare data for OCR model\n",
    "        X = alphabets_df_processed.drop(columns=['label']).values\n",
    "        y = pd.get_dummies(alphabets_df_processed['label']).values\n",
    "\n",
    "        # Split into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Reshape for CNN\n",
    "        X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "        X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "        # Build OCR model\n",
    "        model = Sequential([\n",
    "            Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(len(valid_labels), activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)\n",
    "\n",
    "        # Evaluate model\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print(f\"OCR Model Accuracy: {accuracy}\")\n",
    "\n",
    "        return model\n",
    "    else:\n",
    "        print(\"No data available for training the OCR model.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7896d2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load and preprocess target images for OCR model\n",
    "def preprocess_target_images(target_images_dir):\n",
    "    target_images_files = [os.path.join(target_images_dir, file) for file in os.listdir(target_images_dir) if file.endswith('.png')]\n",
    "    target_images = []\n",
    "\n",
    "    for file in target_images_files:\n",
    "        image = load_img(file, color_mode='grayscale', target_size=(28, 28))\n",
    "        image = img_to_array(image)\n",
    "        target_images.append(image)\n",
    "\n",
    "    target_images = np.array(target_images)\n",
    "    return target_images, target_images_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b659e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manasvigampa/Documents/Vscode/myenv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m5026/5026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 32ms/step - accuracy: 0.7916 - loss: 1.3847 - val_accuracy: 0.9564 - val_loss: 0.1589\n",
      "Epoch 2/5\n",
      "\u001b[1m5026/5026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 43ms/step - accuracy: 0.9615 - loss: 0.1349 - val_accuracy: 0.9579 - val_loss: 0.1518\n",
      "Epoch 3/5\n",
      "\u001b[1m5026/5026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 28ms/step - accuracy: 0.9725 - loss: 0.0939 - val_accuracy: 0.9610 - val_loss: 0.1534\n",
      "Epoch 4/5\n",
      "\u001b[1m5026/5026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - accuracy: 0.9793 - loss: 0.0699 - val_accuracy: 0.9606 - val_loss: 0.1622\n",
      "Epoch 5/5\n",
      "\u001b[1m5026/5026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - accuracy: 0.9830 - loss: 0.0583 - val_accuracy: 0.9650 - val_loss: 0.1714\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9659 - loss: 0.1675\n",
      "OCR Model Accuracy: 0.9649786353111267\n"
     ]
    }
   ],
   "source": [
    "# Train OCR model\n",
    "ocr_model = train_ocr_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a747c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.50      0.50      0.50         2\n",
      "       Happy       0.50      0.50      0.50         2\n",
      "     Neutral       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.67      0.67      0.67         6\n",
      "weighted avg       0.67      0.67      0.67         6\n",
      "\n",
      "         file sentiment predicted_label predicted_sentiment\n",
      "0  line_1.png     Angry               T               Angry\n",
      "1  line_2.png     Angry               Y               Angry\n",
      "2  line_3.png     Happy               T               Angry\n",
      "3  line_4.png     Happy               T               Angry\n",
      "4  line_5.png   Neutral               T               Angry\n",
      "5  line_6.png   Neutral               Y               Angry\n"
     ]
    }
   ],
   "source": [
    "if ocr_model:\n",
    "    target_images_dir = 'target_images'\n",
    "    target_images, target_images_files = preprocess_target_images(target_images_dir)\n",
    "\n",
    "    # Predict on target images\n",
    "    target_predictions = ocr_model.predict(target_images)\n",
    "    target_predictions_classes = np.argmax(target_predictions, axis=1)\n",
    "    valid_labels = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "    target_predicted_labels = [valid_labels[i] for i in target_predictions_classes]\n",
    "\n",
    "    # Convert predicted OCR labels into a string (assuming the labels form words/sentences)\n",
    "    predicted_text = ''.join(target_predicted_labels)\n",
    "\n",
    "    # Map extracted labels to the corresponding files\n",
    "    target_labels['predicted_label'] = [target_predicted_labels[target_images_files.index(os.path.join(target_images_dir, file))] for file in target_labels['file']]\n",
    "\n",
    "    # Drop rows with missing values in 'line' column\n",
    "    sentiment_df = sentiment_df.dropna(subset=['line'])\n",
    "\n",
    "    # Keep rows with valid sentiment values ('Angry', 'Happy', 'Neutral')\n",
    "    valid_sentiments = ['Angry', 'Happy', 'Neutral']\n",
    "    sentiment_df = sentiment_df[sentiment_df['sentiment'].isin(valid_sentiments)]\n",
    "\n",
    "    # Ensure the dataset is not empty after filtering\n",
    "    if sentiment_df.empty:\n",
    "        raise ValueError(\"No data available for training the sentiment analysis model.\")\n",
    "\n",
    "    # Prepare data for Sentiment Analysis model\n",
    "    X_sentiment = sentiment_df['line'].values\n",
    "    y_sentiment = sentiment_df['sentiment'].values\n",
    "\n",
    "    # Vectorize text data using CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_sentiment_vectorized = vectorizer.fit_transform(X_sentiment)\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train_sentiment, X_test_sentiment, y_train_sentiment, y_test_sentiment = train_test_split(X_sentiment_vectorized, y_sentiment, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train Naive Bayes classifier\n",
    "    sentiment_model = MultinomialNB()\n",
    "    sentiment_model.fit(X_train_sentiment, y_train_sentiment)\n",
    "\n",
    "    # Evaluate Sentiment Analysis model\n",
    "    y_pred_sentiment = sentiment_model.predict(X_test_sentiment)\n",
    "    print(classification_report(y_test_sentiment, y_pred_sentiment))\n",
    "\n",
    "    # Predict the sentiment for each extracted label using the sentiment analysis model\n",
    "    target_labels['predicted_sentiment'] = [sentiment_model.predict(vectorizer.transform([label]))[0] for label in target_labels['predicted_label']]\n",
    "\n",
    "    print(target_labels)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
